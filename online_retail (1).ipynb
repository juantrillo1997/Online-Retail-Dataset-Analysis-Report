{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Retail Dataset Analysis Report\n",
    "\n",
    "## Dataset Loading and Initial Overview\n",
    "\n",
    "To start the analysis, we loaded the dataset from the file 'Online Retail.xlsx' and created a copy named `df_original`. Here are the initial steps taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Online Retail.xlsx'\n",
    "df_original = pd.read_excel(file_path, sheet_name='Online Retail')\n",
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial exploration\n",
    "#df.info()\n",
    "\n",
    "# Handling missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# We notice that only details from the description or customer number are missing, not from the \n",
    "# stock number. Additionally, we see that for the rest, there are no missing values, therefore \n",
    "# it is not even necessary to apply dropna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon loading the dataset, we observed that missing values were primarily confined to the 'Description' and 'CustomerID' columns, while the 'StockCode' column had no missing values. \n",
    "\n",
    "```python\n",
    "#Handling missing values\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "Given that other columns showed no missing values, there was no need to apply further data cleaning techniques for missing values (`dropna()`). Columns such as 'InvoiceNo', 'Description', and 'CustomerID' are dropped from the DataFrame (`df`) using `.drop(columns=columns_to_drop)`. These columns are considered unnecessary for subsequent analysis tasks.\n",
    "\n",
    "#### Country Column Adjustment\n",
    "- The value 'United Kingdom' in the 'Country' column is replaced with 'UK' using `.replace()`. This standardizes the country names for consistency in analysis and reporting.\n",
    "\n",
    "These data preparation steps ensure that the DataFrame (`df`) is streamlined and suitable for further exploratory data analysis or modeling tasks in the context of online retail analysis. Note also that the column 'StockCode' includes both strings and integers, and it would actually be necessary to standardize all of them to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns for analysis\n",
    "columns_to_drop = ['InvoiceNo','Description', 'CustomerID']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Filter rows where 'Quantity' or 'UnitPrice' is zero or non-sense for articles\n",
    "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] >= 0.01)]\n",
    "df['Country'].replace('United Kingdom', 'UK', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_stats = df.describe()\n",
    "basic_stats\n",
    "\n",
    "# There is a lot of standard deviation in quantities and prices\n",
    "# Great amount of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Outliers Using Interquartile Range (IQR) \n",
    "\n",
    "\n",
    "- **We give a theorical computation. Later, we will state the concrete code**\n",
    "\n",
    "\n",
    "\n",
    "Based on the statistical summary provided:\n",
    "\n",
    "- **Quantity:**\n",
    "  - Q1 (25th percentile): 1.00\n",
    "  - Q3 (75th percentile): 10.00\n",
    "  - IQR (Interquartile Range): $10.00 - 1.00 = 9.00$\n",
    "\n",
    "  To identify outliers:\n",
    "  - Values below $Q1 - 1.5 \\times IQR$: $1.00 - 1.5 \\times 9.00 = -12.50$ (not applicable since Quantity cannot be negative)\n",
    "  - Values above $Q3 + 1.5 \\times IQR$: $10.00 + 1.5 \\times 9.00 = 24.50$\n",
    "\n",
    "  \n",
    "  \n",
    "  ######  Therefore, any Quantity value above 24.50 could be considered statistically as an outlier.\n",
    "\n",
    "\n",
    "\n",
    "- **UnitPrice:**\n",
    "  - Q1 (25th percentile): 1.25\n",
    "  - Q3 (75th percentile): 4.13\n",
    "  - IQR (Interquartile Range): $4.13 - 1.25 = 2.88$\n",
    "\n",
    "  To identify outliers:\n",
    "  - Values below $Q1 - 1.5 \\times IQR$: $1.25 - 1.5 \\times 2.88 = -2.875$ (not applicable since UnitPrice cannot be negative)\n",
    "  - Values above $Q3 + 1.5 \\times IQR$: $4.13 + 1.5 \\times 2.88 = 8.88$\n",
    "\n",
    "\n",
    "\n",
    "######  Therefore, any UnitPrice value above 8.88 could be considered statistically as an outlier.\n",
    "\n",
    "- These thresholds are based on the interquartile range (IQR) method for outlier detection, providing a guideline to identify potential outliers in the 'Quantity' and 'UnitPrice' columns. Adjustments to these thresholds may be necessary depending on specific business context and data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the interquartile range (IQR) for Quantity\n",
    "Q1 = df['Quantity'].quantile(0.25)\n",
    "Q3 = df['Quantity'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter outliers in Quantity\n",
    "filtered_df_IQR = df[(df['Quantity'] >= Q1 - 1.5 * IQR) & (df['Quantity'] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "# Calculate the interquartile range (IQR) for UnitPrice\n",
    "Q1_price = df['UnitPrice'].quantile(0.25)\n",
    "Q3_price = df['UnitPrice'].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "\n",
    "# Filter outliers in UnitPrice\n",
    "filtered_df_IQR = filtered_df_IQR[(filtered_df_IQR['UnitPrice'] >= Q1_price - 1.5 * IQR_price) & \n",
    "                          (filtered_df_IQR['UnitPrice'] <= Q3_price + 1.5 * IQR_price)]\n",
    "\n",
    "stats_IQR = filtered_df_IQR.describe()\n",
    "stats_IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Isolation Forest on IQR Filtered Data\n",
    "\n",
    "We have previously applied multiple outlier detection methods to our dataset, including the Interquartile Range (IQR), Isolation Forest, Z-Score, and Tukey's method. After reviewing the descriptive statistics generated by these methods, we observed that the IQR method provided the most reliable and representative data.\n",
    "\n",
    "To further refine our dataset and ensure robustness in outlier detection, we will now train the Isolation Forest model on the data that has already been filtered using the IQR method. This approach aims to enhance our outlier detection by leveraging the strengths of both statistical and machine learning techniques.\n",
    "\n",
    "Hereâ€™s how we will proceed:\n",
    "\n",
    "1. **Create a copy of the IQR-filtered DataFrame**:\n",
    "   - This step ensures that our original IQR-filtered data remains intact for future reference.\n",
    "\n",
    "2. **Train the Isolation Forest model**:\n",
    "   - We will configure the Isolation Forest with a contamination parameter of 5% to identify potential outliers.\n",
    "\n",
    "3. **Fit the model and predict outliers**:\n",
    "   - The model will be trained on the 'Quantity' and 'UnitPrice' columns of the IQR-filtered data. It will label each entry as an outlier or inlier.\n",
    "\n",
    "4. **Filter out identified outliers**:\n",
    "   - Entries identified as outliers by the Isolation Forest will be excluded from our dataset, providing a cleaner and more accurate representation of our data.\n",
    "\n",
    "5. **Analyze the refined dataset**:\n",
    "   - We will generate descriptive statistics for the refined dataset to validate the effectiveness of our combined outlier detection approach.\n",
    "\n",
    "By combining the statistical rigor of the IQR method with the machine learning capabilities of the Isolation Forest, we aim to achieve a more robust and accurate dataset for subsequent analysis and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame filtered by the IQR method\n",
    "df_IQR_fit = filtered_df_IQR.copy()\n",
    "\n",
    "# Create the Isolation Forest model\n",
    "clf_over_IQR = IsolationForest(contamination=0.05)  # Specify contamination parameter\n",
    "\n",
    "# Fit the model and predict outliers for 'Quantity' and 'UnitPrice'\n",
    "df_IQR_fit['Outlier_IF'] = clf_over_IQR.fit_predict(df_IQR_fit[['Quantity', 'UnitPrice']])\n",
    "\n",
    "# Filter the DataFrame to exclude outliers (where 'Outlier_IF' == 1)\n",
    "filtered_df_isolated_forest_over_IQR = df_IQR_fit[df_IQR_fit['Outlier_IF'] == 1].drop(columns=['Outlier_IF'])\n",
    "df_cleaned = filtered_df_isolated_forest_over_IQR.copy()\n",
    "\n",
    "# Calculate the descriptive statistics of the filtered DataFrame\n",
    "stats_isolated_forest_over_IQR = filtered_df_isolated_forest_over_IQR.describe()\n",
    "\n",
    "stats_isolated_forest_over_IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the Isolation Forest algorithm to detect and remove outliers from the dataset, the statistical summary of 'Quantity' and 'UnitPrice' shows improvements in their standard deviations.\n",
    "\n",
    "The standard deviation (std) measures the variability or spread of data points around the mean. A lower standard deviation post Isolation Forest indicates that the data points are more tightly clustered around the mean, suggesting reduced variability and improved data consistency. This improvement enhances the reliability of statistical analyses and insights derived from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Quantity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned['Quantity'], bins=50, kde=True)\n",
    "plt.title('Distribution of Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of UnitPrice\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned['UnitPrice'], bins=50, kde=True)\n",
    "plt.title('Distribution of UnitPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of Quantity vs UnitPrice\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Quantity', y='UnitPrice', data=df_cleaned)\n",
    "plt.title('Quantity vs UnitPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\n",
    "\n",
    "df_cleaned['Month'] = df_cleaned['InvoiceDate'].dt.month\n",
    "df_cleaned['DayOfWeek'] = df_cleaned['InvoiceDate'].dt.dayofweek\n",
    "\n",
    "monthly_sales = df_cleaned.groupby('Month')['Quantity'].sum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_sales.plot(kind='bar')\n",
    "plt.title('Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.show()\n",
    "\n",
    "daily_sales = df_cleaned.groupby('DayOfWeek')['Quantity'].sum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "daily_sales.plot(kind='bar')\n",
    "plt.title('Sales by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Productos y paÃ­ses mÃ¡s vendidos\n",
    "# Productos mÃ¡s vendidos\n",
    "top_products = df_cleaned.groupby('StockCode')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_products.plot(kind='bar')\n",
    "plt.title('Top-Selling Products')\n",
    "plt.xlabel('StockCode')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.show()\n",
    "\n",
    "# Crear el grÃ¡fico de pastel\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_products.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Top-Selling Products')\n",
    "plt.ylabel('')  # Eliminar el nombre del eje y\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_codes_top_products = top_products.index.tolist()\n",
    "filtered_df = df_original[['Description', 'StockCode']]\n",
    "filtered_df = filtered_df[filtered_df['StockCode'].isin(stock_codes_top_products)]\n",
    "filtered_df=filtered_df.drop_duplicates(subset='StockCode')\n",
    "filtered_df['Occurrences'] = filtered_df['StockCode'].apply(lambda x: count_stockcode_occurrences(top_products, x))\n",
    "filtered_df = filtered_df.sort_values(by='Occurrences',ascending = False).reset_index(drop=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from Sales Analysis:\n",
    "\n",
    "- **November Sales Peak:**\n",
    "  November emerges as the month with the highest sales volume, indicating robust customer activity and transactional throughput during this period.\n",
    "\n",
    "- **Thursday Sales Dominance:**\n",
    "  Thursdays stand out as the top-performing day of the week in terms of sales, consistently generating the highest sales volume compared to other weekdays.\n",
    "\n",
    "- **Top-Selling Products:**\n",
    "  The products with StockCodes 84879, 85099B, and 85123A are identified as the top-sellers based on their consistently high sales quantities. These items are particularly popular among customers, highlighting their significant contribution to overall sales.\n",
    "  \n",
    "### Top 10 Selling Products with Descriptions:\n",
    "\n",
    "| Index | Description                              | StockCode |\n",
    "|-------|------------------------------------------|-----------|\n",
    "| 0     | ASSORTED COLOUR BIRD ORNAMENT             | 84879     |\n",
    "| 1     | JUMBO BAG RED RETROSPOT                   | 85099B    |\n",
    "| 2     | WHITE HANGING HEART T-LIGHT HOLDER        | 85123A    |\n",
    "| 3     | LUNCH BAG RED RETROSPOT                   | 20725     |\n",
    "| 4     | JAM MAKING SET PRINTED                    | 22961     |\n",
    "| 5     | mailout                                  | 23203     |\n",
    "| 6     | HEART OF WICKER SMALL                     | 22469     |\n",
    "| 7     | SMALL POPCORN HOLDER                      | 22197     |\n",
    "| 8     | SET OF 4 PANTRY JELLY MOULDS              | 22993     |\n",
    "| 9     | LUNCH BAG SUKI DESIGN                     | 22383     |\n",
    "\n",
    "\n",
    "These insights are pivotal for strategic decision-making, enabling the optimization of marketing efforts, inventory management, and customer engagement strategies to capitalize on peak sales periods and popular products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country and sum the quantities\n",
    "country_sales = df_cleaned.groupby('Country')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Plotting histogram of quantity sold by country\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=country_sales.values, y=country_sales.index, palette='viridis')\n",
    "plt.title('Top 10 Countries by Quantity Sold')\n",
    "plt.xlabel('Total Quantity Sold')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country and sum the quantities, excluding UK\n",
    "country_sales = df_cleaned[df_cleaned['Country'] != 'UK'].groupby('Country')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Plotting histogram of quantity sold by country excluding UK\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=country_sales.values, y=country_sales.index, palette='viridis')\n",
    "plt.title('Top 10 Countries by Quantity Sold (Excluding UK)')\n",
    "plt.xlabel('Total Quantity Sold')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only UK data\n",
    "uk_sales = df_cleaned[df_cleaned['Country'] == 'UK']\n",
    "\n",
    "# Get the top-selling products in the UK\n",
    "top_products_uk = uk_sales.groupby('StockCode')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "top_products_uk\n",
    "\n",
    "# Plot the histogram of top-selling products in the UK\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_products_uk.index, y=top_products_uk.values, palette='muted')\n",
    "plt.title('Top 10 Selling Products in the UK')\n",
    "plt.xlabel('StockCode')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only Germany data\n",
    "germany_sales = df_cleaned[df_cleaned['Country'] == 'Germany']\n",
    "\n",
    "# Get the top-selling products in Germany\n",
    "top_products_germany = germany_sales.groupby('StockCode')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "top_products_germany\n",
    "\n",
    "# Plot the histogram of top-selling products in Germany\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_products_germany.index, y=top_products_germany.values, palette='muted')\n",
    "plt.title('Top 10 Selling Products in Germany')\n",
    "plt.xlabel('StockCode')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only France data\n",
    "france_sales = df_cleaned[df_cleaned['Country'] == 'France']\n",
    "\n",
    "# Get the top-selling products in France\n",
    "top_products_france = france_sales.groupby('StockCode')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "top_products_france\n",
    "\n",
    "# Plot the histogram of top-selling products in France\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_products_france.index, y=top_products_france.values, palette='muted')\n",
    "plt.title('Top 10 Selling Products in France')\n",
    "plt.xlabel('StockCode')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the disproportionate size of the UK market compared to other countries means that the top-selling products in the UK have a significant impact on overall market trends. For example, while the top-selling products in Germany and France may differ entirely, the products that dominate in the UK heavily influence the top-selling items across the entire market.\n",
    "\n",
    "This is primarily due to the UK market contributing a substantial portion of the total sales data. Therefore, products that are popular in the UK can skew the overall market trends, leading them to appear as top-selling items in the dataset analysis.\n",
    "\n",
    "From a practical perspective, this underscores the importance of considering the influence of the UK market when analyzing or making decisions based on the top-selling products across all countries. Products consistently favored in the UK are likely to play a pivotal role in strategic business decisions and marketing strategies aimed at optimizing overall sales performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
